{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8791c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import sys \n",
    "sys.path.insert(1, os.path.dirname(os.getcwd()))\n",
    "from functions import *\n",
    "import numpy as np \n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from tensorflow.keras.utils import get_custom_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c5b896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "epochs = 5000\n",
    "train_size, val_size = 80, 40\n",
    "look_back, look_fwd = 10, 10\n",
    "batch_size = 8\n",
    "nodes, kernel_size = 128, 1\n",
    "activation = 'tanh'\n",
    "\n",
    "autoencoder = tf.keras.saving.load_model('cae.keras') \n",
    "\n",
    "# Physical constants\n",
    "const_dict = load_constants()\n",
    "Uf, P, T_h, T_0, Pr, Ra = get_model_constants(const_dict)\n",
    "\n",
    "############################################\n",
    "\n",
    "# CREATE CONTEXT BUILDER\n",
    "def get_context_builder(hidden_size, kernel_size):\n",
    "  inputs = tf.keras.layers.Input(shape=(None, 16, 16, 64), name='Inputs')\n",
    "  _, h1, c1 = ConvLSTM2D(hidden_size, kernel_size, return_sequences=True, return_state=True, padding='same', name='ConvLSTM_CB')(inputs)\n",
    "  return tf.keras.Model(inputs, [h1, c1], name='ContextBuilder_Model')\n",
    "\n",
    "# CREATE SEQUENCE GENERATOR\n",
    "@register_keras_serializable()\n",
    "class SequenceGenerator(tf.keras.Model):\n",
    "  def __init__(self, hidden_size, kernel_size, out_size):\n",
    "    super(SequenceGenerator, self).__init__(name='SequenceGenerator_Model')\n",
    "    self.hidden_size = hidden_size\n",
    "    self.kernel_size = kernel_size\n",
    "    self.out_size = out_size\n",
    "    self.rnn = ConvLSTM2D(hidden_size, kernel_size, return_state=True, padding='same', name='ConvLSTM_SG')\n",
    "    self.conv = Conv2D(64, kernel_size=3, padding='same', name='Conv2D') \n",
    "    self.norm = LayerNormalization(name='Norm')\n",
    "    self.act = LeakyReLU(0.2, name='ReLU')\n",
    "    self.expand_dim = Lambda(lambda x: tf.expand_dims(x, axis=1))\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    initial_input_shape = input_shape[0]\n",
    "    h_shape = c_shape = (1, 16, 16, self.hidden_size)\n",
    "\n",
    "    dummy_input = tf.zeros(initial_input_shape)\n",
    "    dummy_h = tf.zeros(h_shape)\n",
    "    dummy_c = tf.zeros(c_shape)\n",
    "\n",
    "    dec_o, _, _ = self.rnn(dummy_input, initial_state=[dummy_h, dummy_c])\n",
    "    _ = self.act(self.norm(self.conv(dec_o)))\n",
    "    super().build(input_shape)\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    initial_input, h, c, targets, autoreg_prob = inputs\n",
    "    T = tf.shape(targets)[1]\n",
    "    t_switch = tf.cast(T, tf.float32) * autoreg_prob\n",
    "    outputs = tf.TensorArray(dtype=tf.float32, size=T)\n",
    "    input_at_t = initial_input\n",
    "      \n",
    "    def cond_autoreg(t, input_at_t, h, c, outputs):\n",
    "      return tf.cast(t, tf.float32) < t_switch\n",
    "      \n",
    "    def body_autoreg(t, input_at_t, h, c, outputs):\n",
    "      dec_o, h, c = self.rnn(input_at_t, initial_state=[h, c])\n",
    "      output = self.act(self.norm(self.conv(dec_o)))\n",
    "      outputs = outputs.write(t, output)\n",
    "      input_at_t = self.expand_dim(output)\n",
    "      return t + 1, input_at_t, h, c, outputs\n",
    "      \n",
    "    def cond_teacher(t, input_at_t, h, c, outputs):\n",
    "      return tf.cast(t, tf.float32) < tf.cast(T, tf.float32)\n",
    "\n",
    "    def body_teacher(t, input_at_t, h, c, outputs):\n",
    "      dec_o, h, c = self.rnn(input_at_t, initial_state=[h, c])\n",
    "      output = self.act(self.norm(self.conv(dec_o)))\n",
    "      outputs = outputs.write(t, output)\n",
    "      input_at_t = targets[:, t:t+1]\n",
    "      return t + 1, input_at_t, h, c, outputs\n",
    "\n",
    "    t = tf.constant(0)\n",
    "    shape_invs = [t.get_shape(), tf.TensorShape([None, None, 16, 16, self.out_size]), tf.TensorShape([None, 16, 16, 128]), tf.TensorShape([None, 16, 16, 128]), tf.TensorShape(None)]\n",
    "    t, input_at_t, h, c, outputs = tf.while_loop(cond_autoreg, body_autoreg, loop_vars=[t, input_at_t, h, c, outputs], shape_invariants=shape_invs)\n",
    "    t, input_at_t, h, c, outputs = tf.while_loop(cond_teacher, body_teacher, loop_vars=[t, input_at_t, h, c, outputs], shape_invariants=shape_invs)\n",
    "    return tf.transpose(outputs.stack(), perm=[1,0,2,3,4])  \n",
    "\n",
    "  def get_config(self):    \n",
    "    config = super().get_config()\n",
    "    config.update({\"hidden_size\": self.hidden_size, \"kernel_size\": self.kernel_size, 'out_size':self.out_size})\n",
    "    return config\n",
    "    \n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    return cls(**config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f122f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "# LOAD DATA AND dx, dz, dt FOR DERIVATIVES\n",
    "data_train, data_val, x, z = load_lstm_data(train_size, val_size, look_back, look_fwd, Uf, P, T_h, T_0, seqs_train=16, seqs_val=4, autoencoder=autoencoder)               \n",
    "dx_np, dz_np, dt_np = get_grads(x, z, const_dict, Uf)\n",
    "\n",
    "dx = tf.constant(dx_np, tf.float32)\n",
    "dz = tf.constant(dz_np, tf.float32)\n",
    "dt = tf.constant(np.array(dt_np).reshape(1,), tf.float32)\n",
    "\n",
    "\n",
    "# HELPER FUNCTIONS TO COMPUTE DERIVATIVES\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[1,look_fwd,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[1], dtype=tf.float32)]) \n",
    "def DT_tf(var, dt):\n",
    "  ddt1 = (var[...,1:2,:,:,:] - var[...,:1,:,:,:]) / dt\n",
    "  ddt = (var[...,2:,:,:,:] - var[...,:-2,:,:,:]) / (2*dt)\n",
    "  ddt2 = (var[...,-2:-1,:,:,:] - var[...,-1:,:,:,:]) / (-dt)\n",
    "  ddt = tf.concat([ddt1,ddt,ddt2], axis=-4)\n",
    "  return ddt\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[1,look_fwd,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[256], dtype=tf.float32)]) \n",
    "def DX_tf(var, dx):\n",
    "  dx = tf.reshape(dx, [1,1,tf.shape(dx)[0],1,1])\n",
    "  ddx1 = var[...,1:2,:,:] - var[...,:1,:,:]\n",
    "  ddx = var[...,2:,:,:] - var[...,:-2,:,:]\n",
    "  ddx2 = var[...,-2:-1,:,:] - var[...,-1:,:,:]\n",
    "  ddx = tf.concat([ddx1, ddx, ddx2], axis=-3)\n",
    "  return ddx / dx \n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[1,look_fwd,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[256], dtype=tf.float32)])  \n",
    "def DZ_tf(var, dz):\n",
    "  dz = tf.reshape(dz, [1,1,1,tf.shape(dz)[0],1])\n",
    "  ddz1 = var[...,:,1:2,:] - var[...,:,:1,:]\n",
    "  ddz = var[...,:,2:,:] - var[...,:,:-2,:]\n",
    "  ddz2 = var[...,:,-2:-1,:] - var[...,:,-1:,:]\n",
    "  ddz = tf.concat([ddz1,ddz,ddz2], axis=-2)\n",
    "  return ddz / dz \n",
    "\n",
    "#PHYSICS LOSS WRT MASS, MOMENTUM, ENERGY CONSERVATION\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[1,look_fwd,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[1,look_fwd,256,256,4], dtype=tf.float32)])\n",
    "def loss_ns(U_true, U_pred): \n",
    "  \n",
    "  loss_data = tf.reduce_mean(tf.math.square(U_pred-U_true), axis=[0,1,2,3])  \n",
    "  Ld_u = loss_data[0]\n",
    "  Ld_w = loss_data[1]\n",
    "  Ld_p = loss_data[2]\n",
    "  Ld_T = loss_data[3]\n",
    "  \n",
    "  U_pred_x = DX_tf(U_pred, dx)\n",
    "  U_pred_z = DZ_tf(U_pred, dz)\n",
    "  U_pred_t  = DT_tf(U_pred, dt)\n",
    "  U_pred_xx = DX_tf(U_pred_x, dx)\n",
    "  U_pred_zz = DZ_tf(U_pred_z, dz)  \n",
    "\n",
    "  u, w, p, T = tf.split(U_pred, 4, axis=-1)\n",
    "  u_x, w_x, p_x, T_x = tf.split(U_pred_x, 4, axis=-1)\n",
    "  u_z, w_z, p_z, T_z = tf.split(U_pred_z, 4, axis=-1)\n",
    "  u_t, w_t, _, T_t = tf.split(U_pred_t, 4, axis=-1)\n",
    "  u_xx, w_xx, _, T_xx = tf.split(U_pred_xx, 4, axis=-1)\n",
    "  u_zz, w_zz, _, T_zz = tf.split(U_pred_zz, 4, axis=-1)\n",
    "  \n",
    "  f_mc = u_x + w_z     \n",
    "  f_u = u_t + u*u_x + w*u_z + p_x - tf.math.sqrt(Pr/Ra)*(u_xx + u_zz)\n",
    "  f_w = w_t + u*w_x + w*w_z + p_z - tf.math.sqrt(Pr/Ra)*(w_xx + w_zz) - T\n",
    "  f_T = T_t + u*T_x + w*T_z - (T_xx + T_zz)/tf.math.sqrt(Pr*Ra)\n",
    "    \n",
    "  L_mc = tf.reduce_mean(tf.math.square(f_mc))\n",
    "  L_u = tf.reduce_mean(tf.math.square(f_u))\n",
    "  L_w = tf.reduce_mean(tf.math.square(f_w))\n",
    "  L_T = tf.reduce_mean(tf.math.square(f_T))\n",
    "  \n",
    "  return Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T\n",
    "     \n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "\n",
    "context_builder = get_context_builder(hidden_size=nodes, kernel_size=kernel_size)\n",
    "sequence_generator = SequenceGenerator(hidden_size=nodes, kernel_size=kernel_size, out_size=64)\n",
    "ae_decoder = build_ae_decoder(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4351078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION STEP HELPER FUNCTION TO KEEP TRACK OF VALIDATION LOSS\n",
    "low_dims = 256 // (2**4)  \n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[1, look_back, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                            tf.TensorSpec(shape=[1, look_fwd, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                            tf.TensorSpec(shape=[1, look_fwd, 256, 256, 4], dtype=tf.float32),\n",
    "                            tf.TensorSpec(shape=[], dtype=tf.float32)])\n",
    "def val_step_pinn(x_batch, x_dec, U_batch, autoreg_prob):\n",
    "  h, c = context_builder(x_batch, training=False)\n",
    "  x = sequence_generator((x_batch[:,-1:], h, c, x_dec, autoreg_prob), training=False)\n",
    "  U_pred = ae_decoder(x, training=False)\n",
    "  \n",
    "  Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T = loss_ns(U_batch, U_pred) \n",
    "  return tf.stack([Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T], axis=0)\n",
    "\n",
    "# HELPER FUNCTION FOR TRAIN STEP FOR EACH BATCH\n",
    "## Batch size of 1, gradients are accumulated to simulate larger batch size due to memory constraints\n",
    "input_signature_train = [tf.TensorSpec(shape=[1, look_back, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[1, look_fwd, low_dims, low_dims, 64], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[1, look_fwd, 256, 256, 4], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[8], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[2], dtype=tf.float32),\n",
    "                       tf.TensorSpec(shape=[], dtype=tf.float32)]\n",
    "\n",
    " \n",
    "@tf.function(input_signature=input_signature_train)\n",
    "def train_step_pinn(x_batch, x_dec, U_batch, l_dwa, l_g, autoreg_prob):\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    h, c = context_builder(x_batch, training=True)\n",
    "    x = sequence_generator((x_batch[:,-1:], h, c, x_dec, autoreg_prob), training=True)\n",
    "    U_pred = ae_decoder(x, training=False)\n",
    "    \n",
    "    Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T = loss_ns(U_batch, U_pred)\n",
    "    loss_data_pde = tf.stack([Ld_u, Ld_w, Ld_p, Ld_T, L_mc, L_u, L_w, L_T], axis=0)\n",
    "    \n",
    "    ldata = (l_dwa[0]*Ld_u + l_dwa[1]*Ld_w + l_dwa[2]*Ld_p + l_dwa[3]*Ld_T)\n",
    "    lpde = (l_dwa[4]*L_mc + l_dwa[5]*L_u + l_dwa[6]*L_w + l_dwa[7]*L_T)\n",
    "    loss = ldata + lpde\n",
    "    \n",
    "  grad_data = tape.gradient(ldata, context_builder.trainable_variables + sequence_generator.trainable_variables)\n",
    "  grad_pde = tape.gradient(lpde, context_builder.trainable_variables + sequence_generator.trainable_variables)\n",
    "  \n",
    "  return loss_data_pde, grad_data, grad_pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68ec5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER VARIABLES FOR TRAINING\n",
    "loss_history = []\n",
    "l_g = tf.Variable([1., 1.], dtype=tf.float32, trainable=False) \n",
    "w = tf.constant(0.9, dtype=tf.float32)\n",
    "l_dwa = tf.Variable(tf.ones([8], tf.float32) / 8, trainable=False)\n",
    " \n",
    "best_loss = float('inf')\n",
    "patience = 20\n",
    "decay_rate = 0.8\n",
    "\n",
    "dwa_steps = tf.Variable(16, dtype=tf.int32, trainable=False)\n",
    "acc_steps = tf.Variable(4, dtype=tf.int32, trainable=False)\n",
    "\n",
    "\n",
    "grad_norm = tf.Variable(tf.zeros([2], tf.float32), trainable=False)\n",
    "grad_norm_mean = tf.Variable(0., trainable=False)\n",
    "\n",
    "grad_data_acc = [tf.Variable(tf.zeros_like(v), trainable=False) for v in context_builder.trainable_variables]\n",
    "grad_data_acc += [tf.Variable(tf.zeros_like(v), trainable=False) for v in sequence_generator.trainable_variables]\n",
    "\n",
    "grad_pde_acc = [tf.Variable(tf.zeros_like(v), trainable=False) for v in context_builder.trainable_variables]\n",
    "grad_pde_acc += [tf.Variable(tf.zeros_like(v), trainable=False) for v in sequence_generator.trainable_variables]\n",
    "\n",
    "grad_acc = [tf.Variable(tf.zeros_like(v), trainable=False) for v in context_builder.trainable_variables]\n",
    "grad_acc += [tf.Variable(tf.zeros_like(v), trainable=False) for v in sequence_generator.trainable_variables]\n",
    "\n",
    "# WE USE TEACHER FORCING WITH SCHEDULED SAMPLING\n",
    "## The training starts with the model predicting one step ahead. \n",
    "## Gradually we expose the model to longer output sequences up to the full 60\n",
    "rec_prob = tf.Variable(0., trainable=False)\n",
    "cur_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "rec_prob_val = tf.constant(1.)\n",
    "\n",
    "stop_teach = look_fwd*20\n",
    "warmup = 250\n",
    "learning_rate = 1-3\n",
    "min_lr = 1e-4\n",
    "wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d095aa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (1.0e+00, 9.0e-04), Data:5.19e-02, MC:4.18e+05, u:3.76e+05, w:4.81e+05, T:5.87e+05, V Data:4.93e-02, V MC:3.75e+05, V u:4.54e+05, V w:5.45e+05, V T:4.83e+05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (x_train, x_dec_train, U_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_train):\n\u001b[0;32m     10\u001b[0m   cur_step\u001b[38;5;241m.\u001b[39massign(step)\n\u001b[1;32m---> 12\u001b[0m   Ldata_pde_b, grad_data_b, grad_pde_b \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_pinn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_dwa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m   Ldata_pde \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Ldata_pde_b\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;28mlen\u001b[39m(grad_acc)):\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\lucam\\Anaconda3\\envs\\py12\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN TRAINING\n",
    "for epoch in tf.range(1, epochs, dtype=tf.float32):\n",
    "  \n",
    "  rec_prob.assign( tf.minimum(1., epoch / stop_teach) )\n",
    "  \n",
    "  Ldata_pde = tf.zeros([8], tf.float32)\n",
    "  val_loss = tf.zeros([8], tf.float32)\n",
    "    \n",
    "  for step, (x_train, x_dec_train, U_train) in enumerate(data_train):\n",
    "    cur_step.assign(step)\n",
    "    \n",
    "    Ldata_pde_b, grad_data_b, grad_pde_b = train_step_pinn(x_train, x_dec_train, U_train, l_dwa, l_g, rec_prob)\n",
    "    Ldata_pde += Ldata_pde_b\n",
    "    \n",
    "    for i in tf.range(len(grad_acc)):\n",
    "      grad_data_acc[i].assign_add(grad_data_b[i] / tf.cast(acc_steps, tf.float32) )\n",
    "      grad_pde_acc[i].assign_add(grad_pde_b[i] / tf.cast(acc_steps, tf.float32) )\n",
    "      \n",
    "    loss_history.append(Ldata_pde_b)\n",
    "\n",
    "    # Loss balancing & gradient accumulation\n",
    "    if (cur_step+1) % acc_steps == 0: \n",
    "    \n",
    "      dwa_rollmean = tf.reduce_mean(loss_history[-dwa_steps:-1], axis=0)\n",
    "      l_dwa.assign( tf.nn.softmax(tf.cast(loss_history[-1] / dwa_rollmean, tf.float32)) )\n",
    "      \n",
    "      grad_data_norm = tf.linalg.global_norm(grad_data_acc)\n",
    "      grad_pde_norm = tf.linalg.global_norm(grad_pde_acc)\n",
    "      \n",
    "      grad_norm.assign( tf.stack([grad_data_norm, grad_pde_norm], axis=0) )\n",
    "      \n",
    "      l_g_hat = tf.stack([tf.constant(1.) / grad_data_norm, tf.constant(1.) / grad_pde_norm])\n",
    "      l_g.assign( w*l_g + (1-w)*l_g_hat )\n",
    "      l_g.assign(l_g / tf.reduce_sum(l_g))\n",
    "      \n",
    "      for i in tf.range(len(grad_acc)):\n",
    "        grad_acc[i].assign( l_g[0]*grad_data_acc[i] + l_g[1]*grad_pde_acc[i] )\n",
    "      \n",
    "      max_grad_norm = tf.constant(100.0) * grad_data_norm \n",
    "      gradients, _ = tf.clip_by_global_norm(grad_acc, max_grad_norm)\n",
    "      optimizer.apply_gradients(zip(gradients, context_builder.trainable_variables + sequence_generator.trainable_variables))\n",
    "      \n",
    "      for i in tf.range(len(grad_acc)):\n",
    "        grad_acc[i].assign(tf.zeros_like(grad_pde_b[i]))\n",
    "        grad_data_acc[i].assign(tf.zeros_like(grad_pde_b[i]))\n",
    "        grad_pde_acc[i].assign(tf.zeros_like(grad_pde_b[i]))\n",
    "        \n",
    "      grad_norm.assign(tf.zeros_like(grad_norm))\n",
    "      \n",
    "\n",
    "  Ldata_pde /= (step+1)\n",
    "  for step_val, (x_val, x_dec_val, U_val) in enumerate(data_val):      \n",
    "    val_loss_b = val_step_pinn(x_val, x_dec_val, U_val, rec_prob_val)\n",
    "    val_loss = tf.math.add_n([val_loss, val_loss_b])\n",
    "    \n",
    "  val_loss /= (step_val+1)\n",
    "  val_data_loss = tf.reduce_mean(val_loss[:4])\n",
    "  val_pde_loss = val_loss[-4:]\n",
    "\n",
    "  # Learning rate scheduler\n",
    "  if epoch >= warmup:\n",
    "    if (best_loss - tf.reduce_mean(val_loss)) > 1e-3: \n",
    "      best_loss = tf.reduce_mean(val_loss)\n",
    "      wait = 0\n",
    "    else:\n",
    "      wait += 1\n",
    "      \n",
    "    if wait >= patience:\n",
    "      new_lr = max(learning_rate * decay_rate, min_lr)\n",
    "      if new_lr < learning_rate:\n",
    "        learning_rate = new_lr\n",
    "        optimizer.learning_rate.assign(learning_rate)\n",
    "        \n",
    "      wait = 0\n",
    "   \n",
    "  log1 = f\"{tf.cast(epoch, tf.int32)}. Data:{tf.reduce_mean(Ldata_pde[:4]):.2e}, \"\n",
    "  log2 = f\"MC:{Ldata_pde[4]:.2e}, u:{Ldata_pde[5]:.2e}, w:{Ldata_pde[6]:.2e}, T:{Ldata_pde[7]:.2e}, V Data:{val_data_loss:.2e}, \"\n",
    "  log3 = f\"V MC:{val_pde_loss[0]:.2e}, V u:{val_pde_loss[1]:.2e}, V w:{val_pde_loss[2]:.2e}, V T:{val_pde_loss[3]:.2e}\"\n",
    "  \n",
    "  print(log1+log2+log3)\n",
    "\n",
    "# save trained temporal model\n",
    "context_builder.save('context_builder.keras', overwrite=True)\n",
    "sequence_generator.save_weights('sequence_generator.weights.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
