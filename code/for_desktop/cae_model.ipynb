{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3f33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "import sys \n",
    "sys.path.insert(1, os.path.dirname(os.getcwd()))\n",
    "from functions import *\n",
    "import numpy as np \n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ff19a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL HYPERPARAMETERS\n",
    "epochs = 5 #20000\n",
    "train_size = 80 \n",
    "val_size = 20\n",
    "kernel_size = 3\n",
    "nodes = [64,64,64,64]\n",
    "activation = 'linear'\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "min_lr = 0.0001\n",
    "\n",
    "# LOADING TRAINING AND VALIDATION DATA\n",
    "const_dict = load_constants() \n",
    "Uf, P, T_h, T_0, Pr, Ra = get_model_constants(const_dict) \n",
    "ae_train, ae_val, x, z = load_ae_data(train_size, val_size, batch_size, Uf, P, T_h, T_0)\n",
    "\n",
    "\n",
    "# LOSS FUNCTION (MSE)\n",
    "# 4 components are separated for loss dynamic loss balancing\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32)])\n",
    "def my_loss(U_true, U_pred):\n",
    "  losses = tf.reduce_mean(tf.math.square(U_pred-U_true), axis=[0,1,2])\n",
    "  return losses[0], losses[1], losses[2], losses[3]\n",
    "  \n",
    "# BUILD MODEL\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "autoencoder = build_ae(nodes, kernel_size, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23936f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN STEP FUNCTION\n",
    "## For each batch we compute balanced loss\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[4], dtype=tf.float32)])\n",
    "def train_step(x_batch, U_batch, lambdas):\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    U_pred = autoencoder(x_batch, training=True)\n",
    "    losses = my_loss(U_batch, U_pred) \n",
    "    loss_u, loss_w, loss_p, loss_T = losses[0], losses[1], losses[2], losses[3] \n",
    "    loss = loss_u*lambdas[0] + loss_w*lambdas[1] + loss_p*lambdas[2] + loss_T*lambdas[3]\n",
    "    \n",
    "  loss_data = tf.stack([loss_u, loss_w, loss_p, loss_T], axis=0)\n",
    "  gradients = tape.gradient(loss, autoencoder.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables)) \n",
    "  return loss_data\n",
    "\n",
    "# VALIDATION STEP FUNCTION\n",
    "## Tracking validation loss\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32),\n",
    "                              tf.TensorSpec(shape=[batch_size,256,256,4], dtype=tf.float32)])\n",
    "def val_step(x_batch, U_batch):\n",
    "  U_pred = autoencoder(x_batch, training=False)\n",
    "  losses = my_loss(U_batch, U_pred) \n",
    "  loss_u, loss_w, loss_p, loss_T = losses[0], losses[1], losses[2], losses[3] \n",
    "  return (loss_u + loss_w + loss_p + loss_T) / 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "945cc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER VARIABLES DURING TRAINING\n",
    "learning_rate = lr\n",
    "best_loss = float('inf') \n",
    "lambdas = tf.Variable(tf.ones([4], tf.float32) / 4, trainable=False)\n",
    "loss_history = tf.Variable(tf.zeros([4]), dtype=tf.float32, trainable=False)\n",
    "\n",
    "Ld = tf.Variable(tf.ones([4]), dtype=tf.float32, trainable=False)\n",
    "step_tf = tf.Variable(0., dtype=tf.float32, trainable=False) \n",
    "step_val_tf = tf.Variable(0., dtype=tf.float32, trainable=False) \n",
    "w = tf.Variable(tf.zeros([4]), dtype=tf.float32, trainable=False) \n",
    "val_loss = tf.Variable(0., dtype=tf.float32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed96ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.47e-01, Val Loss: 1.03e-01, (u): 4.67e-01, (w): 4.13e-01, (p): 4.60e-01, (T): 4.46e-01\n",
      "Epoch 2, Loss: 6.98e-02, Val Loss: 4.44e-02, (u): 7.78e-02, (w): 6.78e-02, (p): 6.17e-02, (T): 7.19e-02\n",
      "Epoch 3, Loss: 3.70e-02, Val Loss: 3.06e-02, (u): 4.26e-02, (w): 4.49e-02, (p): 1.98e-02, (T): 4.07e-02\n",
      "Epoch 4, Loss: 2.73e-02, Val Loss: 2.39e-02, (u): 2.53e-02, (w): 3.86e-02, (p): 1.23e-02, (T): 3.30e-02\n",
      "Epoch 5, Loss: 2.22e-02, Val Loss: 2.03e-02, (u): 1.69e-02, (w): 3.35e-02, (p): 9.37e-03, (T): 2.90e-02\n"
     ]
    }
   ],
   "source": [
    "# RUN TRAINING\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  # loop over the training data\n",
    "  for step, (x_batch, U_batch) in enumerate(ae_train):\n",
    "    step_tf.assign(step)\n",
    "    Ld_temp = train_step(x_batch, U_batch, lambdas)\n",
    "    Ld.assign_add(Ld_temp)\n",
    "       \n",
    "  Ld.assign( Ld / (step_tf+1.) )\n",
    "  Ldata = tf.math.reduce_mean(Ld)\n",
    "  \n",
    "  if epoch >= 1:\n",
    "    w.assign(Ld / loss_history)  \n",
    "    lambdas.assign(tf.nn.softmax(w))\n",
    "  loss_history.assign(Ld)\n",
    "\n",
    "  # loop over validation data\n",
    "  for step_val, (x_batch_val, U_batch_val) in enumerate(ae_val):\n",
    "    step_val_tf.assign(step_val)\n",
    "    val_loss.assign_add( val_step(x_batch_val, U_batch_val) )\n",
    "\n",
    "  val_loss.assign( val_loss / (step_val_tf+1.) )\n",
    "\n",
    "  # LEARNING RATE SCHEDULER IF VALIDATION LOSS PLATEAUS\n",
    "  if epoch >= 100:\n",
    "    if (best_loss - val_loss) > 1e-4: \n",
    "      best_loss = val_loss\n",
    "      wait = 0\n",
    "    else:\n",
    "      wait += 1\n",
    "      \n",
    "    if wait >= 20:\n",
    "      new_lr = max(learning_rate * 0.8, min_lr)\n",
    "      if new_lr < learning_rate:\n",
    "        learning_rate = new_lr\n",
    "        optimizer.learning_rate.assign(learning_rate)\n",
    "      wait = 0\n",
    "      \n",
    "  log = f\"Epoch {epoch+1}, Loss: {Ldata.numpy():.2e}, Val Loss: {val_loss.numpy():.2e}, \"\n",
    "  log2 = f\"(u): {Ld[0].numpy():.2e}, (w): {Ld[1].numpy():.2e}, (p): {Ld[2].numpy():.2e}, (T): {Ld[3].numpy():.2e}\"  \n",
    "  print(log+log2)\n",
    "   \n",
    "  Ld.assign(tf.zeros_like(Ld))  \n",
    "  val_loss.assign(tf.constant(0.))  \n",
    "    \n",
    "autoencoder.save_weights('cae.weights.h5', overwrite=True)\n",
    "autoencoder.save('cae.keras', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
